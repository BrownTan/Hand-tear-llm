{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总结一下就是：\n",
    "* 参数多了num_key_value_heads，代表有多少kv头\n",
    "* k_proj和v_proj的输出维度不再是hidden_size（num_heads * head_size），而是num_key_value_heads * head_size\n",
    "* k，v做.view()的时候也要注意，需要用num_key_value_heads而不是num_heads\n",
    "* 因为维度不匹配，k，v需要在dim=1（即num_key_value_heads维度）上做repeat_interleave，重复张量中的元素\n",
    "* 最后再view时需要先contiguous使内存连续化，但reshape不用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 当num_key_value_heads == 1时，为MQA\n",
    "# 忽略attention_mask，attention_dropout\n",
    "class GroupQueryAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, num_key_value_heads):\n",
    "        super().__init__()\n",
    "        assert hidden_size % num_heads == 0\n",
    "        assert num_heads % num_key_value_heads == 0\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = hidden_size // num_heads\n",
    "        self.num_key_value_heads = num_key_value_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.k_proj = nn.Linear(hidden_size, num_key_value_heads * self.head_size)\n",
    "        self.v_proj = nn.Linear(hidden_size, num_key_value_heads * self.head_size)\n",
    "        self.o_proj = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None, head_mask=None):\n",
    "        batch_size, seq_len, _ = hidden_states.size()\n",
    "\n",
    "        q_states = self.q_proj(hidden_states)\n",
    "        k_states = self.k_proj(hidden_states)\n",
    "        v_states = self.v_proj(hidden_states)\n",
    "\n",
    "        q_states = q_states.view(batch_size, seq_len, self.num_heads, self.head_size).transpose(1, 2)\n",
    "        k_states = k_states.view(batch_size, seq_len, self.num_key_value_heads, self.head_size).transpose(1, 2)\n",
    "        v_states = v_states.view(batch_size, seq_len, self.num_key_value_heads, self.head_size).transpose(1, 2)\n",
    "\n",
    "        # k,v repeat for each group\n",
    "        # torch will not broadcast the repeat dim automatically\n",
    "        # repeat_interleave 沿着指定维度重复张量中的元素n次\n",
    "        k_states = k_states.repeat_interleave(self.num_heads // self.num_key_value_heads, dim=1)\n",
    "        v_states = v_states.repeat_interleave(self.num_heads // self.num_key_value_heads, dim=1)\n",
    "\n",
    "        # compute attention scores\n",
    "        attention_scores = torch.matmul(q_states, k_states.transpose(-1, -2)) / math.sqrt(self.head_size)\n",
    "        if attention_mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(attention_mask == 0, float(\"-inf\"))\n",
    "        attention_scores = torch.softmax(attention_scores, dim=-1)\n",
    "        attention_scores = torch.matmul(attention_scores, v_states)\n",
    "\n",
    "        output = self.o_proj(attention_scores.transpose(1, 2).reshape(batch_size, seq_len, self.hidden_size))\n",
    "        # output = self.o_proj(attention_scores.transpose(1, 2).view(batch_size, seq_len, self.hidden_size))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 128])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试\n",
    "x = torch.randn(3, 2, 128)\n",
    "net = GroupQueryAttention(128, 8, 4)\n",
    "net(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tanshuai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
