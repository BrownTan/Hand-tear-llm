{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 手搓\n",
    "有点艰难，主要还是pytorch维度问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLoRALayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, rank, lora_alpha, dropout_rate, merge=False):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.rank = rank\n",
    "        self.lora_alpha = lora_alpha\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.merge = merge\n",
    "\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        # self.linear.weight 的 shape 是 (out_features, in_features)\n",
    "\n",
    "        if rank > 0:\n",
    "            # A and B is not linear layers, but parameters only.\n",
    "            # self.lora_A = nn.Linear(in_features, rank, bias=False)\n",
    "            # # 高斯分布\n",
    "            # nn.init.normal_(self.lora_A.weight, mean=0.0, std=1.0)\n",
    "            # self.lora_B = nn.Linear(rank, out_features, bias=False)\n",
    "\n",
    "            self.lora_A = nn.Parameter(torch.empty(rank, in_features))\n",
    "            self.lora_B = nn.Parameter(torch.empty(out_features, rank))\n",
    "            nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.lora_B)\n",
    "            self.scaling = self.lora_alpha / self.rank\n",
    "\n",
    "        self.linear.weight.requires_grad = False\n",
    "        self.dropout = nn.Dropout(dropout_rate) if dropout_rate > 0 else nn.Identity()\n",
    "\n",
    "        # merge\n",
    "        if merge:\n",
    "            self.linear.weight.data += self.lora_B @ self.lora_A * self.scaling\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.rank > 0:\n",
    "            print(x.shape, x.T.shape)\n",
    "            output_part1 = self.linear(x)\n",
    "            output_part2 = (x @ (self.lora_B @ self.lora_A).T) * self.scaling\n",
    "            print(output_part1.shape, output_part2.shape)\n",
    "            output = output_part1 + output_part2\n",
    "        else:\n",
    "            output = self.linear(x)\n",
    "\n",
    "        output = self.dropout(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape (no merge): torch.Size([32, 128, 512])\n",
      "Output shape (no merge): torch.Size([32, 128, 512])\n"
     ]
    }
   ],
   "source": [
    "# 写一段测试代码\n",
    "# Test the LoRALinear layer\n",
    "batch_size = 32\n",
    "seq_len = 128\n",
    "in_features = 768\n",
    "out_features = 512\n",
    "rank = 8\n",
    "lora_alpha = 16\n",
    "dropout = 0.1\n",
    "\n",
    "# Create a test input\n",
    "x = torch.randn(batch_size, seq_len, in_features)\n",
    "\n",
    "# Test regular mode (no merge)\n",
    "lora_layer = LinearLoRALayer(\n",
    "    in_features=in_features,\n",
    "    out_features=out_features,\n",
    "    rank=rank,\n",
    "    lora_alpha=lora_alpha,\n",
    "    dropout_rate=dropout,\n",
    "    merge=False\n",
    ")\n",
    "\n",
    "# Forward pass\n",
    "output = lora_layer(x)\n",
    "print(f\"Output shape (no merge): {output.shape}\")  # Should be [batch_size, seq_len, out_features]\n",
    "\n",
    "# Test regular mode (no merge)\n",
    "lora_layer = LinearLoRALayer(\n",
    "    in_features=in_features,\n",
    "    out_features=out_features,\n",
    "    rank=rank,\n",
    "    lora_alpha=lora_alpha,\n",
    "    dropout_rate=dropout,\n",
    "    merge=True\n",
    ")\n",
    "\n",
    "# Forward pass\n",
    "output = lora_layer(x)\n",
    "print(f\"Output shape (no merge): {output.shape}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.7793e-07, 0.0000e+00, 1.4013e-45,  ..., 0.0000e+00, 3.7793e-07,\n",
       "         0.0000e+00],\n",
       "        [3.3631e-44, 6.5711e+05, 1.1502e-07,  ..., 1.2125e+25, 4.7426e+30,\n",
       "         1.7237e+25],\n",
       "        [3.6434e-44, 5.6052e-45, 3.6434e-44,  ..., 1.8058e+28, 3.7793e-07,\n",
       "         0.0000e+00],\n",
       "        ...,\n",
       "        [1.5975e-43, 0.0000e+00, 3.7862e-07,  ..., 0.0000e+00, 3.7863e-07,\n",
       "         0.0000e+00],\n",
       "        [1.5975e-43, 0.0000e+00, 3.7862e-07,  ..., 0.0000e+00, 3.7863e-07,\n",
       "         0.0000e+00],\n",
       "        [1.5975e-43, 0.0000e+00, 3.7862e-07,  ..., 0.0000e+00, 3.7863e-07,\n",
       "         0.0000e+00]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.tensor() 只创建torch.FloatTensor类型的张量，是torch.empty() 的特例，empty（）返回一个包含未初始化数据的张量\n",
    "torch.empty(in_features, rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = nn.Linear(3, 4)\n",
    "linear.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.7755, -4.0443,  1.3888,  3.9942])\n",
      "tensor([-0.7755, -4.0443,  1.3888,  3.9942])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, 4)\n",
    "y = torch.randn(3)\n",
    "print(x.T @ y)\n",
    "print(y @ x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# github 官方代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer():\n",
    "    def __init__(\n",
    "        self, \n",
    "        r: int, \n",
    "        lora_alpha: int, \n",
    "        lora_dropout: float,\n",
    "        merge_weights: bool,\n",
    "    ):\n",
    "        self.r = r\n",
    "        self.lora_alpha = lora_alpha\n",
    "        # Optional dropout\n",
    "        if lora_dropout > 0.:\n",
    "            self.lora_dropout = nn.Dropout(p=lora_dropout)\n",
    "        else:\n",
    "            self.lora_dropout = lambda x: x\n",
    "        # Mark the weight as unmerged\n",
    "        self.merged = True\n",
    "        self.merge_weights = merge_weights\n",
    "\n",
    "class Linear(nn.Linear, LoRALayer):\n",
    "    # LoRA implemented in a dense layer\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_features: int, \n",
    "        out_features: int, \n",
    "        r: int = 0, \n",
    "        lora_alpha: int = 1, \n",
    "        lora_dropout: float = 0.,\n",
    "        fan_in_fan_out: bool = False, # Set this to True if the layer to replace stores weight like (fan_in, fan_out)\n",
    "        merge_weights: bool = True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        nn.Linear.__init__(self, in_features, out_features, **kwargs)\n",
    "        LoRALayer.__init__(self, r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout,\n",
    "                           merge_weights=merge_weights)\n",
    "\n",
    "        self.fan_in_fan_out = fan_in_fan_out\n",
    "        # Actual trainable parameters\n",
    "        if r > 0:\n",
    "            self.lora_A = nn.Parameter(self.weight.new_zeros((r, in_features)))\n",
    "            self.lora_B = nn.Parameter(self.weight.new_zeros((out_features, r)))\n",
    "            self.scaling = self.lora_alpha / self.r\n",
    "            # Freezing the pre-trained weight matrix\n",
    "            self.weight.requires_grad = False\n",
    "        self.reset_parameters()\n",
    "        if fan_in_fan_out:\n",
    "            self.weight.data = self.weight.data.transpose(0, 1)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.Linear.reset_parameters(self)\n",
    "        if hasattr(self, 'lora_A'):\n",
    "            # initialize B the same way as the default for nn.Linear and A to zero\n",
    "            # this is different than what is described in the paper but should not affect performance\n",
    "            nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.lora_B)\n",
    "\n",
    "    def train(self, mode: bool = True):\n",
    "        def T(w):\n",
    "            return w.transpose(0, 1) if self.fan_in_fan_out else w\n",
    "        nn.Linear.train(self, mode)\n",
    "        if mode:\n",
    "            if self.merge_weights and self.merged:\n",
    "                # Make sure that the weights are not merged\n",
    "                if self.r > 0:\n",
    "                    self.weight.data -= T(self.lora_B @ self.lora_A) * self.scaling\n",
    "                    print(self.weight.data.shape)\n",
    "                    print(self.lora_B.shape)\n",
    "                    print(self.lora_A.shape)\n",
    "                    print((self.lora_B @ self.lora_A).shape)\n",
    "                    print((T(self.lora_B @ self.lora_A)).shape)\n",
    "                self.merged = False\n",
    "        else:\n",
    "            if self.merge_weights and not self.merged:\n",
    "                # Merge the weights and mark it\n",
    "                if self.r > 0:\n",
    "                    self.weight.data += T(self.lora_B @ self.lora_A) * self.scaling\n",
    "                self.merged = True       \n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # oh, 原来如此\n",
    "        def T(w):\n",
    "            return w.transpose(0, 1) if self.fan_in_fan_out else w\n",
    "        \n",
    "        if self.r > 0 and not self.merged:\n",
    "            result = F.linear(x, T(self.weight), bias=self.bias)            \n",
    "            result += (self.lora_dropout(x) @ self.lora_A.transpose(0, 1) @ self.lora_B.transpose(0, 1)) * self.scaling\n",
    "            return result\n",
    "        else:\n",
    "            return F.linear(x, T(self.weight), bias=self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape (no merge): torch.Size([32, 128, 512])\n",
      "torch.Size([512, 768])\n",
      "torch.Size([512, 8])\n",
      "torch.Size([8, 768])\n",
      "torch.Size([512, 768])\n",
      "torch.Size([512, 768])\n",
      "Output shape (no merge): torch.Size([32, 128, 512])\n"
     ]
    }
   ],
   "source": [
    "# 写一段测试代码\n",
    "# Test the LoRALinear layer\n",
    "batch_size = 32\n",
    "seq_len = 128\n",
    "in_features = 768\n",
    "out_features = 512\n",
    "rank = 8\n",
    "lora_alpha = 16\n",
    "dropout = 0.1\n",
    "\n",
    "# Create a test input\n",
    "x = torch.randn(batch_size, seq_len, in_features)\n",
    "\n",
    "# Test regular mode (no merge)\n",
    "lora_layer = Linear(\n",
    "    in_features=in_features,\n",
    "    out_features=out_features,\n",
    "    r=rank,\n",
    "    lora_alpha=lora_alpha,\n",
    "    merge_weights=False\n",
    ")\n",
    "\n",
    "lora_layer.train()\n",
    "\n",
    "# Forward pass\n",
    "output = lora_layer(x)\n",
    "print(f\"Output shape (no merge): {output.shape}\")  # Should be [batch_size, seq_len, out_features]\n",
    "\n",
    "# Test regular mode (no merge)\n",
    "lora_layer = Linear(\n",
    "    in_features=in_features,\n",
    "    out_features=out_features,\n",
    "    r=rank,\n",
    "    lora_alpha=lora_alpha,\n",
    "    merge_weights=True\n",
    ")\n",
    "\n",
    "lora_layer.train()\n",
    "\n",
    "# Forward pass\n",
    "output = lora_layer(x)\n",
    "print(f\"Output shape (no merge): {output.shape}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'T' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape, x\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mshape, \u001b[43mT\u001b[49m(x)\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'T' is not defined"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3)\n",
    "print(x.shape, x.T.shape, T(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tanshuai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
