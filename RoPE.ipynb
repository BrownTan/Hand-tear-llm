{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rotary Positional Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "llama代码细读：https://dingfen.github.io/2023/10/30/2023-10-30-huggingface1/\n",
    "\n",
    "自注意力机制本身是位置不敏感的，需要加上位置信息。\n",
    "\n",
    "一般来说，绝对位置编码具有实现简单、计算速度快等优点，而相对位置编码则直接地体现了相对位置信号，跟我们的直观理解吻合，实际性能往往也更好。\n",
    "\n",
    "RoPE：通过绝对位置编码的方式实现相对位置编码。特性为：上下文表示与旋转矩阵相乘来编码相对位置；可扩展到任意长度；可用于线性注意力机制；词间距离与依赖性相关。\n",
    "\n",
    "看苏剑林博客：https://kexue.fm/archives/8265 ，迷迷糊糊"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{R}_{\\Theta,m} ^d x = \n",
    "\\begin{pmatrix}\n",
    "x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ \\vdots \\\\ x_{d-1} \\\\ x_d\n",
    "\\end{pmatrix}\n",
    "\\otimes\n",
    "\\begin{pmatrix}\n",
    "\\cos m \\theta_1 \\\\ \\cos m \\theta_1 \\\\ \\cos m \\theta_2 \\\\ \\cos m \\theta_2 \\\\ \\vdots \\\\ \\cos m \\theta_{d/2} \\\\ \\cos m \\theta_{d/2}\n",
    "\\end{pmatrix}\n",
    "+\n",
    "\\begin{pmatrix} \n",
    "-x_2 \\\\ x_1 \\\\ -x_4 \\\\ x_3 \\\\ \\vdots \\\\ -x_{d-1} \\\\ x_d\n",
    "\\end{pmatrix}\n",
    "\\otimes\n",
    "\\begin{pmatrix}\n",
    "\\sin m \\theta_1 \\\\ \\sin m \\theta_1 \\\\ \\sin m \\theta_2 \\\\ \\sin m \\theta_2 \\\\ \\vdots \\\\ \\sin m \\theta_{d/2} \\\\ \\sin m \\theta_{d/2}\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\theta_{i} = 10000^{-2(i-1)/d}, \\quad i \\in [1, 2, \\ldots, d/2]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个讲的不错：\n",
    "\n",
    "【通俗易懂-大模型的关键技术之一：旋转位置编码rope （2）】 https://www.bilibili.com/video/BV1Tr421p7By/?share_source=copy_web&vd_source=308c6ef1763d60b08057708fbfe7c230"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama\n",
    "其实llama现在的modeling.py用的也不是这种了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)] / dim))  # [dim / 2]\n",
    "    # print(freqs.shape)\n",
    "    t = torch.arange(end, device=freqs.device)  # [end]\n",
    "    freqs = torch.outer(t, freqs).float()  # [end , dim / 2]\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # 将极坐标表示转换为直角坐标的复数。两参数作为复数的模和角度（以弧度为单位）\n",
    "    # 也就得到了cos(m·theta(i))+sin(m·theta(i)) j, m=0,1,...,end-1, i=0,1,...,dim/2-1 \n",
    "    return freqs_cis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_for_broadcast(freqs_cis, x):\n",
    "    ndim = x.ndim\n",
    "    assert ndim > 1\n",
    "    # print(freqs_cis.shape, x.shape)\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    return freqs_cis.view(*shape)  # [1, seq_len, 1, head_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_embedding(q, k, freqs_cis):\n",
    "    q_ = torch.view_as_complex(q.float().reshape(*q.shape[:-1], -1, 2))  # 变成复数，[batch, seq_len, head_num, head_dim/2]\n",
    "    k_ = torch.view_as_complex(k.float().reshape(*k.shape[:-1], -1, 2))  \n",
    "    freqs_cis = reshape_for_broadcast(freqs_cis, q_)  # 可以跟q_, k_相乘了\n",
    "    # print(q_.device, freqs_cis.device)\n",
    "    q_out = torch.view_as_real(q_ * freqs_cis).flatten(2)  # 变回实数，[batch, seq_len, head_num, head_dim]\n",
    "    k_out = torch.view_as_real(k_ * freqs_cis).flatten(2)  # 有head_num则flatten(3)\n",
    "    # 举个例子，(x(1) + x(2)j) * (cos(m·theta(1))+sin(m·theta(1)) j) \n",
    "    #          == (x(1)cos(m·theta(1)) - x(2)sin(m·theta(1))) + (x(1)sin(m·theta(1)) + x(2)cos(m·theta(1)))j \n",
    "    # 再变回实数，上面公式的前两行就得到了\n",
    "    return q_out.type_as(q), k_out.type_as(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4])\n",
      "torch.Size([4, 4]) torch.Size([2, 4, 4])\n",
      "torch.Size([2, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "# 测试rope\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "head_dim = 8\n",
    "q = torch.randn(batch_size, seq_len, head_dim)\n",
    "k = torch.randn(batch_size, seq_len, head_dim)\n",
    "\n",
    "q_rope, k_rope =apply_rotary_embedding(q, k, precompute_freqs_cis(head_dim, seq_len))\n",
    "print(q_rope.shape)  # 输出形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  0.,  0.],\n",
      "        [10., 20., 30.],\n",
      "        [20., 40., 60.]])\n"
     ]
    }
   ],
   "source": [
    "# 向量外积是矩阵\n",
    "t = torch.tensor([0, 1, 2], dtype=torch.float)\n",
    "freqs = torch.tensor([10, 20, 30], dtype=torch.float)\n",
    "freqs = torch.outer(t, freqs)\n",
    "print(freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000+0.0000j,  1.0000+0.0000j,  1.0000+0.0000j],\n",
       "        [-0.8391-0.5440j,  0.4081+0.9129j,  0.1543-0.9880j],\n",
       "        [ 0.4081+0.9129j, -0.6669+0.7451j, -0.9524-0.3048j]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# out = abs · cos(angle) + abs · sin(angle) · j\n",
    "freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
    "freqs_cis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.8390715290764524, -0.5440211108893698)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "1 * math.cos(10), 1 * math.sin(10)  # 测试出torch.polar的公式正确"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2j"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 复数乘法法则：(a+bi)(c+di)=(ac-bd)+(bc+ad)i\n",
    "(1.0000+1.0000j) * (-1-1j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatGLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不一定要按维度顺序两两组合旋转，神经元是无序的，不依赖维度顺序\n",
    "def rotate_half(x):\n",
    "    \"\"\"Rotate half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rope(x, dim):\n",
    "    # print(x.shape)\n",
    "    theta = 1. / (10000 ** (torch.arange(0, dim, 2).float() / dim)).to(x.device)  # [dim / 2]\n",
    "    # print(theta.shape)\n",
    "    seq_len = x.shape[1]\n",
    "    seq_idx = torch.arange(seq_len).float().to(x.device)  # [seq_len]\n",
    "    # print(seq_idx.shape)\n",
    "    idx_theta = torch.einsum('i,j->ij', seq_idx, theta)  # 爱因斯坦求和约定（Einstein summation convention）\n",
    "    # print(idx_theta.shape)\n",
    "    idx_theta2 = torch.cat([idx_theta, idx_theta], dim=1)\n",
    "    # print(idx_theta2.shape)\n",
    "    cos_cached = idx_theta2.cos()[None, :, :]  # [seq_len, head_dim]扩展维度到[seq_len, 1, 1, head_dim]\n",
    "    sin_cached = idx_theta2.sin()[None, :, :]\n",
    "    # sin_cached = idx_theta2.sin()[:, None, None, :]  # 原版是这样的\n",
    "    # print(cos_cached.shape)\n",
    "    x_rope = (x * cos_cached) + (rotate_half(x) * sin_cached)\n",
    "    # print(x_rope.shape)\n",
    "    return x_rope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1419,  0.0721, -0.6689, -0.9504, -1.3637,  1.5168, -1.0835,\n",
      "           0.1435],\n",
      "         [ 0.2478, -0.3760,  0.2059, -0.7293,  0.5550,  0.1493, -1.0972,\n",
      "          -0.1126],\n",
      "         [ 0.0925, -0.9905, -0.6838, -0.3321, -1.2511, -0.6986,  0.2962,\n",
      "          -1.1512],\n",
      "         [ 0.9016,  0.1168,  1.4854,  0.6318,  0.6785, -0.6640,  0.7347,\n",
      "          -1.5072]],\n",
      "\n",
      "        [[ 0.7554, -0.9979, -1.1566,  0.1857,  0.9541, -0.3422,  0.3528,\n",
      "           0.8431],\n",
      "         [ 3.0744,  0.3404,  0.2961,  0.4119,  0.1635,  0.2764, -0.7737,\n",
      "           0.3776],\n",
      "         [-0.2102,  1.1085, -0.9568, -0.0940,  0.2554, -0.7549,  1.1950,\n",
      "          -0.0265],\n",
      "         [ 0.4608, -1.0372,  2.6274, -2.1610, -1.0221, -0.1619,  0.3776,\n",
      "           1.2964]]]) tensor([[[ 0.1419,  0.0721, -0.6689, -0.9504, -1.3637,  1.5168, -1.0835,\n",
      "           0.1435],\n",
      "         [ 0.6269,  0.3115,  0.2667, -0.7063,  0.0894,  0.1870, -1.0991,\n",
      "          -0.1130],\n",
      "         [ 1.4984, -0.6077, -0.5978, -0.4624,  0.4462, -0.4791,  0.3121,\n",
      "          -1.1499],\n",
      "         [ 0.8008, -0.0287,  1.2541,  1.0446, -0.7785, -0.6926,  0.6943,\n",
      "          -1.5070]],\n",
      "\n",
      "        [[ 0.7554, -0.9979, -1.1566,  0.1857,  0.9541, -0.3422,  0.3528,\n",
      "           0.8431],\n",
      "         [ 0.6636,  1.7114,  0.2457,  0.4390, -2.5010,  0.2160, -0.7770,\n",
      "           0.3764],\n",
      "         [-0.9845, -0.0990, -0.8954, -0.2775,  0.1041, -0.9582,  1.2139,\n",
      "          -0.0239],\n",
      "         [ 0.7410,  0.9436,  3.1572, -1.2813,  0.9419,  0.1802,  0.2947,\n",
      "           1.3038]]]) tensor([[[ 0.1419,  0.0721, -0.6689, -0.9504, -1.3637,  1.5168, -1.0835,\n",
      "           0.1435],\n",
      "         [ 0.6269,  0.3115,  0.2667, -0.7063,  0.0894,  0.1870, -1.0991,\n",
      "          -0.1130],\n",
      "         [ 1.4984, -0.6077, -0.5978, -0.4624,  0.4462, -0.4791,  0.3121,\n",
      "          -1.1499],\n",
      "         [ 0.8008, -0.0287,  1.2541,  1.0446, -0.7785, -0.6926,  0.6943,\n",
      "          -1.5070]],\n",
      "\n",
      "        [[ 0.7554, -0.9979, -1.1566,  0.1857,  0.9541, -0.3422,  0.3528,\n",
      "           0.8431],\n",
      "         [ 0.6636,  1.7114,  0.2457,  0.4390, -2.5010,  0.2160, -0.7770,\n",
      "           0.3764],\n",
      "         [-0.9845, -0.0990, -0.8954, -0.2775,  0.1041, -0.9582,  1.2139,\n",
      "          -0.0239],\n",
      "         [ 0.7410,  0.9436,  3.1572, -1.2813,  0.9419,  0.1802,  0.2947,\n",
      "           1.3038]]])\n",
      "torch.Size([2, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "# 测试rope\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "head_dim = 8\n",
    "x = torch.randn(batch_size, seq_len, head_dim)\n",
    "\n",
    "x_rope = rope(x, head_dim)\n",
    "print(x_rope.shape)  # 输出形状\n",
    "\n",
    "# 测试出两种写法结果不一致，原因未知\n",
    "q_rope, k_rope =apply_rotary_embedding(x, x, precompute_freqs_cis(head_dim, seq_len))\n",
    "print(x_rope, q_rope, k_rope)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 冒泡排序\n",
    "公司来人面试，忍不住小试牛刀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解释冒泡排序原理：冒泡排序是一种简单的排序算法，它通过 repeatedly 遍历整个列表，比较相邻的元素，如果发现它们的顺序错误，就将它们交换位置。\n",
    "def bubble_sort(arr):\n",
    "    n = len(arr)\n",
    "    for i in range(n):\n",
    "        for j in range(0, n-i-1):\n",
    "            if arr[j] > arr[j+1] :\n",
    "                arr[j], arr[j+1] = arr[j+1], arr[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "# 测试冒泡排序\n",
    "arr = [3, 5, 1, 2, 4]\n",
    "bubble_sort(arr)\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tanshuai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
